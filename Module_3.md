
<img width="596" alt="Screen Shot 2023-10-29 at 10 47 24 AM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/8812f12b-ba08-43c8-b04c-69f4ef201586">
<img width="600" alt="Screen Shot 2023-10-29 at 10 47 59 AM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/61865761-8a7e-44c1-8033-acc41fceb8b5">

The LLM has the application in predictive typing; speech recognition, grammar correction.

<img width="699" alt="Screen Shot 2023-10-29 at 10 58 35 AM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/ddb7b5c3-085a-4105-a63f-df2e23d88d13">


<img width="715" alt="Screen Shot 2023-10-29 at 12 01 08 PM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/e1b90986-b7cb-4230-bf04-8f9cbad2018d">

In RNN, h_t is a state variable, h_t is a function of the previous state, and the input variable x_t.

<img width="705" alt="Screen Shot 2023-10-29 at 12 07 16 PM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/0ee7c343-a793-49de-9ccb-6b827d5e6b5b">

<img width="706" alt="Screen Shot 2023-10-29 at 12 13 12 PM" src="https://github.com/daichaoyi/DeepLearning_CS7643/assets/50822172/753663b2-f3e3-4f7f-adc4-aa418524e9ed">

'vanilla' RNN is difficult due to vanishing gradients.
$\abs(W_theta)<1$





